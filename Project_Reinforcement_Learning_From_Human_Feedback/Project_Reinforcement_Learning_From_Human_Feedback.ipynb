{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Fine-Tuning (SFT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download dataset from transformers library to contruct formatting like: \"Text: document # Summary: summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install lib\n",
    "!pip install -q datasets evaluate==0.4.1 rouge_score==0.1.2 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "sft_ds_name = 'CarperAI/openai_summarize_tldr'\n",
    "sft_ds = load_dataset(sft_ds_name)\n",
    "sft_train = sft_ds['train']\n",
    "sft_valid = sft_ds['valid']\n",
    "sft_test = sft_ds['test']\n",
    "\n",
    "# contruct\n",
    "def formatting_func(example):\n",
    "    text = f\"### Text: {example['promt']}\\n ### Summary: {example['label']}\"\n",
    "    return text\n",
    "\n",
    "# demo formatting\n",
    "for example in sft_train:\n",
    "    print(formatting_func(example))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prior training model using OPT. So that speed up training model, we can use `quantization` technique and `LORA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import ModelConfig, get_quantization_config, get_kbit_device_map\n",
    "from peft import LoraConfig, PertConfig, PertModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path = 'facebook/opt-350m'\n",
    ")\n",
    "\n",
    "torch_dtype = (\n",
    "    model_config.torch_dtype\n",
    "    if model_config.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_config.torch_dtype)\n",
    ")\n",
    "\n",
    "quantization_config = get_quantization_config(model_config)\n",
    "model_kwargs = dict(\n",
    "    revision = model_config.model_revision,\n",
    "    trust_remote_code = model_config.trust_remote_code,\n",
    "    attn_implementation=model_config.attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_cache=False,\n",
    "    device_map=get_kbit_device_map () if quantization_config is not None else None,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.model_name_or_path, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# lora\n",
    "peft_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Metric\n",
    "\n",
    "We use `ROUGE` metric to evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    if isinstance(eval_preds, tuple):\n",
    "        eval_preds = eval_preds[0]\n",
    "    labels_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids , skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels_ids , skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=pred_str , references=label_str)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Trainer\n",
    "We shall contruct parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 10\n",
    "trainig_args = TrainingArguments(\n",
    "    output_dir = './save_model',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    adam_beta1 =0.9,\n",
    "    adam_beta2 =0.95,\n",
    "    num_train_epochs=num_epochs,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "max_input_length = 512\n",
    "trainer = SFTTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    model_init_kwargs=model_kwargs,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_train,\n",
    "    eval_dataset=sft_valid,\n",
    "    max_seq_length=max_input_length,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    compute_metrics=compute_metrics,\n",
    "    packing=True,\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
